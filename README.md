Custom INT8 Quantizer
Overview
This repository provides a custom implementation of an INT8 quantizer designed to optimize deep learning models for edge deployment. INT8 quantization reduces the model size and improves inference speed while maintaining acceptable accuracy levels.

Features
Custom quantization algorithms for converting floating-point models to INT8.
Supports popular deep learning frameworks such as TensorFlow and PyTorch.
